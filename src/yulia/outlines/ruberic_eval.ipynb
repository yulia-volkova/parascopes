{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a981e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_parallel import exponential_backoff, process_in_parallel\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7073cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECODED_CSV = \"/workspace/ALGOVERSE/yas/yulia/parascopes/src/yulia/outlines/decoded_outputs_chunk_999.csv\"\n",
    "\n",
    "# Hugging Face parquet with the last 100k completions (we only take last 1000)\n",
    "HF_PQ = \"yulia-volkova/parascopes-outlines-data@main/v5.0/data/outlines_009.parquet\"\n",
    "\n",
    "# Output files\n",
    "OUT_DIR = Path(\"./eval_outlines\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "OUT_JSONL_PRED = OUT_DIR / \"rubric_predicted.jsonl\"\n",
    "OUT_JSONL_ORIG = OUT_DIR / \"rubric_original.jsonl\"\n",
    "OUT_SUMMARY_CSV = OUT_DIR / \"rubric_summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba601d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (1000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>dataset_idx</th>\n",
       "      <th>model</th>\n",
       "      <th>completion</th>\n",
       "      <th>outline_generated</th>\n",
       "      <th>reconstructed_text</th>\n",
       "      <th>embedding_id</th>\n",
       "      <th>index</th>\n",
       "      <th>decoded_original</th>\n",
       "      <th>decoded_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>999000</td>\n",
       "      <td>999000</td>\n",
       "      <td>meta-llama/Llama-3.3-70B-Instruct</td>\n",
       "      <td>**The Sugar Maple: From Forest to Farm**\\n\\nTh...</td>\n",
       "      <td>Outline:\\n1. Sugar Maple Characteristics\\n   -...</td>\n",
       "      <td></td>\n",
       "      <td>999000</td>\n",
       "      <td>0</td>\n",
       "      <td>Summary: 1.Sugar maple leaves - valuable chara...</td>\n",
       "      <td>Summary: 1. Design of Sugarcane Branch - 2. Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>999001</td>\n",
       "      <td>999001</td>\n",
       "      <td>meta-llama/Llama-3.3-70B-Instruct</td>\n",
       "      <td>A Legacy of Union: The United Church of Christ...</td>\n",
       "      <td>Outline:\\n1. United Church of Christ history\\n...</td>\n",
       "      <td></td>\n",
       "      <td>999001</td>\n",
       "      <td>1</td>\n",
       "      <td>Description: 1. History of the United Christ C...</td>\n",
       "      <td>Summary: 1. Origin - Church of England 2. Foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>999002</td>\n",
       "      <td>999002</td>\n",
       "      <td>meta-llama/Llama-3.3-70B-Instruct</td>\n",
       "      <td>**Discontinuity Proofs with Epsilon-Delta**\\n\\...</td>\n",
       "      <td>Outline:\\n1. Epsilon-Delta Definition\\n   - Di...</td>\n",
       "      <td></td>\n",
       "      <td>999002</td>\n",
       "      <td>2</td>\n",
       "      <td>Summary: 1.Epsilon-Delta Definition - Disconti...</td>\n",
       "      <td>Summary: 1. Explanation of deferential consona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_id  dataset_idx                              model  \\\n",
       "0      999000       999000  meta-llama/Llama-3.3-70B-Instruct   \n",
       "1      999001       999001  meta-llama/Llama-3.3-70B-Instruct   \n",
       "2      999002       999002  meta-llama/Llama-3.3-70B-Instruct   \n",
       "\n",
       "                                          completion  \\\n",
       "0  **The Sugar Maple: From Forest to Farm**\\n\\nTh...   \n",
       "1  A Legacy of Union: The United Church of Christ...   \n",
       "2  **Discontinuity Proofs with Epsilon-Delta**\\n\\...   \n",
       "\n",
       "                                   outline_generated reconstructed_text  \\\n",
       "0  Outline:\\n1. Sugar Maple Characteristics\\n   -...                      \n",
       "1  Outline:\\n1. United Church of Christ history\\n...                      \n",
       "2  Outline:\\n1. Epsilon-Delta Definition\\n   - Di...                      \n",
       "\n",
       "   embedding_id  index                                   decoded_original  \\\n",
       "0        999000      0  Summary: 1.Sugar maple leaves - valuable chara...   \n",
       "1        999001      1  Description: 1. History of the United Christ C...   \n",
       "2        999002      2  Summary: 1.Epsilon-Delta Definition - Disconti...   \n",
       "\n",
       "                                   decoded_predicted  \n",
       "0  Summary: 1. Design of Sugarcane Branch - 2. Li...  \n",
       "1  Summary: 1. Origin - Church of England 2. Foun...  \n",
       "2  Summary: 1. Explanation of deferential consona...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REF_COL = \"outline_generated\"  \n",
    "\n",
    "# --- Load parquet from HF and take last 1000 ---\n",
    "ds = load_dataset(\"parquet\", data_files=f\"hf://datasets/{HF_PQ}\")[\"train\"]\n",
    "n = len(ds)\n",
    "tail = ds.select(range(n-1000, n))  # last 1000\n",
    "\n",
    "ref_df = tail.to_pandas().reset_index(drop=True)\n",
    "ref_df[\"index\"] = np.arange(1000)  # local 0..999 index within chunk 999\n",
    "\n",
    "if REF_COL not in ref_df.columns:\n",
    "    raise RuntimeError(f\"Column '{REF_COL}' not found in parquet. Available: {ref_df.columns.tolist()}\")\n",
    "\n",
    "dec_df = pd.read_csv(DECODED_CSV)\n",
    "# keep only chunk 999 and needed columns\n",
    "need_cols = {\"chunk\",\"index\",\"decoded_original\",\"decoded_predicted\"}\n",
    "missing = need_cols - set(dec_df.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Decoded CSV missing columns: {missing}\")\n",
    "\n",
    "dec_df = dec_df[dec_df[\"chunk\"] == 999].copy()\n",
    "dec_df = dec_df.sort_values(\"index\").reset_index(drop=True)\n",
    "\n",
    "# --- Merge on 'index' so rows align: original (ref_df[REF_COL]) vs decoded_* ---\n",
    "merged = ref_df.merge(dec_df[[\"index\",\"decoded_original\",\"decoded_predicted\"]], on=\"index\", how=\"inner\")\n",
    "\n",
    "print(\"Merged shape:\", merged.shape)\n",
    "merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cb4c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric = \"\"\"\n",
    "#### 0. Complexity\n",
    "How complex is the outline text?\n",
    "0: Trivial (e.g: just says \"** Section **\")\n",
    "1. Simple (e.g: \"** Section 1: Green Tea **\")\n",
    "2. Some detail (e.g: a short undetailed sentence or two about something)\n",
    "3. Many details (e.g: a detailed paragraph with specific information)\n",
    "\n",
    "#### 1. Coherence (Outline-Level)\n",
    "Does Outline 2 make sense as an outline compared to Outline 1?\n",
    "0: Completely incoherent (e.g., excessive repetition, nonsensical phrases, strange symbols).\n",
    "1: Partially coherent, but repetitive or has formatting issues (e.g., repeated key phrases, awkward pauses).\n",
    "2: Mostly coherent with minor grouping/order issues.\n",
    "3: Clear, logical, coherent outline structure.\n",
    "\n",
    "#### 2. Hierarchy / Structure\n",
    "How well does Outline 2 preserve the hierarchical levels (headings vs sub-bullets)?\n",
    "0: No recognizable hierarchy; flat or malformed.\n",
    "1: Basic levels exist but often mis-nested.\n",
    "2: Mostly correct hierarchy with some mismatches.\n",
    "3: Hierarchy closely matches with minimal deviations, highly similar structure.\n",
    "\n",
    "#### 3. Coverage of Key Sections\n",
    "Do the major sections in Outline 1 appear in Outline 2?\n",
    "0: Most key sections missing or unrelated.\n",
    "1: About half of major sections appear.\n",
    "2: Most sections present; minor omissions.\n",
    "3: All major sections present (allow synonyms/regrouping).\n",
    "\n",
    "#### 4. Ordering / Flow\n",
    "Does the order of major sections and sub-sections follow Outline 1?\n",
    "0: Largely shuffled or illogical.\n",
    "1: Partial overlap but frequent swaps.\n",
    "2: Mostly consistent with minor swaps.\n",
    "3: Order closely matches.\n",
    "\n",
    "#### 5. Subject Match\n",
    "How similar is the subject of Outline 2 to Outline 1?\n",
    "-1: No subjects to compare.\n",
    "0: Completely unrelated subjects (\"corporate law\" vs \"particle physics\")\n",
    "1: Vaguely similar field (e.g: \"biology\" vs \"physics\" are both sciences)\n",
    "2: Related general domain or adjacent fields (e.g., \"history\" vs. \"archaeology\" or \"alternative medicine\" vs. \"traditional remedies\").\n",
    "3: Same subject (e.g., both discuss \"ancient mayans\" or \"the properties of the human body\").\n",
    "4: Identical focus (e.g., both analyze \"ancient mayan architecture\").\n",
    "\n",
    "#### 6. Entities / Key Concepts\n",
    "How well does Outline 2 preserve entities or technical terms from Outline 1?\n",
    "-1: No entities to compare.\n",
    "0: Unrelated entities.\n",
    "1: Same category but little overlap.\n",
    "2: Some overlap or synonyms.\n",
    "3: Most entities/terms preserved.\n",
    "4: Nearly all preserved.\n",
    "\n",
    "#### 7. Details\n",
    "How similar are the details in the response (Outline 2) to the reference (Outline 1)?\n",
    "-1: Neither outline text has details to compare.\n",
    "0: Details differ completely (e.g., Outline 1 lists benefits; Outline 2 is generic).\n",
    "1: Minimal depth (e.g., both mention \"anti-inflammatory properties\" with no specifics).\n",
    "2: Moderate depth (e.g., discuss benefits + 1-2 supporting facts).\n",
    "3: Highly specific details (e.g., \"40% reduction in inflammation\").\n",
    "\n",
    "#### 8. Conciseness of Headings\n",
    "Are headings concise and outline-appropriate (not full sentences)?\n",
    "0: Often verbose, unclear, or sentence-like.\n",
    "1: Mixed clarityâ€”some concise, some verbose.\n",
    "2: Mostly concise, descriptive headings.\n",
    "\n",
    "#### 9. Identical\n",
    "Is Outline 2 essentially identical to Outline 1 (ignoring trivial formatting)?\n",
    "0: Not identical.\n",
    "1: Identical.\n",
    "---\n",
    "\n",
    "JSON output: {\n",
    "    \"reasoning\": {complexity, coherence, hierarchy, coverage, ordering, subject, entities, details, conciseness, identical} - each with explanation string\n",
    "    \"scoring\":  {Same keys as above} - each with number score\n",
    "}\n",
    "\n",
    "Reasoning should be concise, and explicitly state the \"name\" of the level (such\n",
    "as for entities: \"[reasoning about the texts], Thus: similar category and partial identical entities: thus out of 4, score 3\").\n",
    "\n",
    "If the specific category does not apply to many things (e.g: complexity is 0 or\n",
    "1), and there is no specifics between either, then by default give full points.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "json_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"reasoning\": {\n",
    "            \"complexity\": {\"type\": \"string\"},\n",
    "            \"coherence\": {\"type\": \"string\"},\n",
    "            \"hierarchy\": {\"type\": \"string\"},\n",
    "            \"coverage\": {\"type\": \"string\"},\n",
    "            \"ordering\": {\"type\": \"string\"},\n",
    "            \"subject\": {\"type\": \"string\"},\n",
    "            \"entities\": {\"type\": \"string\"},\n",
    "            \"details\": {\"type\": \"string\"},\n",
    "            \"conciseness\": {\"type\": \"string\"},\n",
    "            \"identical\": {\"type\": \"string\"},\n",
    "        },\n",
    "        \"scoring\": {\n",
    "            \"complexity\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 3},\n",
    "            \"coherence\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 3},\n",
    "            \"hierarchy\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 3},\n",
    "            \"coverage\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 3},\n",
    "            \"ordering\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 3},\n",
    "            \"subject\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 4},\n",
    "            \"entities\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 4},\n",
    "            \"details\": {\"type\": \"integer\", \"minimum\": -1, \"maximum\": 3},\n",
    "            \"conciseness\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 2},\n",
    "            \"identical\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 1},\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"reasoning\", \"scoring\"],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "\n",
    "\n",
    "@exponential_backoff\n",
    "def ruberic_compare(ref_text: str, comp_text: str):\n",
    "    \"\"\"\n",
    "    Call the LLM to compare two outlines using the rubric and return JSON (as text).\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Using the following rubric, compare the two outlines:\\n\\n\"\n",
    "        f\"Rubric: {rubric}\\n\\n\"\n",
    "        f\"Outline 1 (reference): {ref_text}\\n\\n\"\n",
    "        f\"Outline 2 (candidate): {comp_text}\\n\\n\"\n",
    "        \"The output must be a valid JSON object and nothing else.\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\")\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-4o-mini\",\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"You are an expert evaluator.\\n\\n\" + prompt\n",
    "        }],\n",
    "        temperature=0.3,\n",
    "        max_tokens=4000,\n",
    "        response_format={\"type\": \"json_object\"},  # enforce JSON\n",
    "        function_call={\"name\": \"validate_json\", \"parameters\": json_schema}\n",
    "    )\n",
    "    # return raw JSON string \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_ruberic_comparison(ref_texts: List[str], comp_texts: List[str], dataset_idxs: List[int], label=None):\n",
    "    results = []\n",
    "    for index, (ref_text, comp_text, ds_idx) in enumerate(zip(ref_texts, comp_texts, dataset_idxs)):\n",
    "        result = ruberic_compare(ref_text, comp_text)\n",
    "        results.append(result)\n",
    "        print({\n",
    "            \"index\": index,\n",
    "            \"dataset_idx\": ds_idx,\n",
    "            \"type\": label,\n",
    "            \"reference\": ref_text,\n",
    "            \"comparison\": comp_text,\n",
    "            \"result\": result\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def get_ruberic_parallel(ref_texts: List[str], comp_texts: List[str], dataset_idxs: List[int], label=None, max_workers: int = 20):\n",
    "\n",
    "    items = list(zip(np.arange(len(ref_texts)), ref_texts, comp_texts, dataset_idxs))\n",
    "    print(f\"Processing {len(items)} comparisons in parallel\")\n",
    "\n",
    "    def get_rubric(item):\n",
    "        index, ref_text, comp_text, ds_idx = item\n",
    "        result = ruberic_compare(ref_text, comp_text)\n",
    "        print({\n",
    "            \"index\": index,\n",
    "            \"dataset_idx\": ds_idx,\n",
    "            \"label\": label,\n",
    "            \"reference\": ref_text,\n",
    "            \"comparison\": comp_text,\n",
    "            \"result\": result\n",
    "        })\n",
    "        sys.stdout.flush()\n",
    "        return result\n",
    "\n",
    "    results = process_in_parallel(items, get_rubric, max_workers=max_workers)\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_outlines_df(\n",
    "    df_merged: pd.DataFrame,\n",
    "    ref_col: str = \"outline_generated\",\n",
    "    cand_col: str = \"decoded_predicted\",\n",
    "    out_csv: str = \"outline_eval_with_rubric.csv\",\n",
    "    parallel_workers: int = 20,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs rubric evaluation on df_merged[ref_col] vs df_merged[cand_col],\n",
    "    stores raw JSON + parsed scores into columns, and saves a CSV.\n",
    "    \"\"\"\n",
    "    assert ref_col in df_merged.columns and cand_col in df_merged.columns, \\\n",
    "        f\"DataFrame must have columns '{ref_col}' and '{cand_col}'\"\n",
    "\n",
    "    ref_texts  = df_merged[ref_col].astype(str).tolist()\n",
    "    comp_texts = df_merged[cand_col].astype(str).tolist()\n",
    "\n",
    "    # get JSON strings (one per row)\n",
    "    raw_results = get_ruberic_parallel(ref_texts, comp_texts, df_merged[\"dataset_idx\"].tolist(), label=\"probe-decoded\", max_workers=parallel_workers)\n",
    "\n",
    "    # attach raw JSON and parsed scores\n",
    "    df_out = df_merged.copy()\n",
    "    df_out[\"rubric_json\"] = raw_results\n",
    "\n",
    "    # parse and expand scores\n",
    "    def _safe_load(js):\n",
    "        try:\n",
    "            return json.loads(js)\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "    parsed = [ _safe_load(s) for s in raw_results ]\n",
    "    # add each score as its own column\n",
    "    score_keys = list(json_schema[\"properties\"][\"scoring\"].keys())\n",
    "    for k in score_keys:\n",
    "        df_out[f\"score_{k}\"] = [p.get(\"scoring\", {}).get(k, None) for p in parsed]\n",
    "\n",
    "    # optional: add a total/average score\n",
    "    # here we sum available scores (treat missing as NaN and skip)\n",
    "    df_out[\"score_sum\"] = df_out[[f\"score_{k}\" for k in score_keys]].apply(\n",
    "        lambda r: np.nansum([float(x) if x is not None else np.nan for x in r.values]), axis=1\n",
    "    )\n",
    "    df_out[\"score_mean\"] = df_out[[f\"score_{k}\" for k in score_keys]].apply(\n",
    "        lambda r: np.nanmean([float(x) if x is not None else np.nan for x in r.values]), axis=1\n",
    "    )\n",
    "\n",
    "    # save\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved rubric evaluation to: {out_csv}\")\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6814a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scored = evaluate_outlines_df(\n",
    "#     merged,\n",
    "#     ref_col=\"outline_generated\",\n",
    "#     cand_col=\"decoded_predicted\",\n",
    "#     out_csv=\"outline_eval_chunk999.csv\",\n",
    "#     parallel_workers=20,\n",
    "# )\n",
    "\n",
    "# score_cols = [c for c in df_scored.columns if c.startswith(\"score_\")]\n",
    "# print(\"Mean scores:\")\n",
    "# print(df_scored[score_cols].mean(numeric_only=True).sort_values(ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yulia-venv)",
   "language": "python",
   "name": "yulia-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
